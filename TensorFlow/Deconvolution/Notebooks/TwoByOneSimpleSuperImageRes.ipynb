{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a86e5e1",
   "metadata": {},
   "source": [
    "# Simple Super Image Resolution Test with the Deconvolutional Layer\n",
    "\n",
    "This notebook will document the process of taking an image dataset blurring with a convolution operation with a 2 by 2 kernel. The blurred image will be taken as an input for the Deconvolution Neural Network where the network will try to predict the unblurred image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4f8bf",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf11efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2144d",
   "metadata": {},
   "source": [
    "## Defining functions for blurring images\n",
    "\n",
    "`get_imgs_from_dir(ds_path, target_size)` gets the image dataset from the specific directory\n",
    "\n",
    "`easy_two_by_two_blur(ds_path)` applies a simple blur to the images in the given path and outputs it to a folder, in the same directory, called two_by_two_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba578203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imgs_from_dir(ds_path, color_mode='grayscale', target_size=[256, 256]):\n",
    "    img_names = os.listdir(ds_path)\n",
    "    imgs = []\n",
    "\n",
    "    for img_name in img_names:\n",
    "        if img_name[-4:] == '.png':\n",
    "            img_path = os.path.join(ds_path, img_name)\n",
    "            img = tf.keras.utils.load_img(img_path,\n",
    "                                          color_mode=color_mode,\n",
    "                                          target_size=target_size)\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            imgs.append(img)\n",
    "    aug_imgs = imgs\n",
    "\n",
    "    imgs_tensor = tf.constant(aug_imgs, dtype=tf.float32)\n",
    "    x_train = imgs_tensor / 255.0\n",
    "\n",
    "    return x_train\n",
    "\n",
    "def easy_two_by_one_blur_ds(ds_path):\n",
    "    # get data that needs to be transformed\n",
    "    print('Loading data')\n",
    "    imgs = get_imgs_from_dir(ds_path, color_mode='rgb')\n",
    "    print('Data Loaded!')\n",
    "    \n",
    "    \n",
    "    # set up convolution kernel\n",
    "    k = np.array([[1],\n",
    "                  [0.5]], dtype=np.float32)\n",
    "    k0 = np.zeros(k.shape, dtype=np.float32)\n",
    "\n",
    "    kr = np.array([k, k0, k0], dtype=np.float32)\n",
    "    kg = np.array([k0, k, k0], dtype=np.float32)\n",
    "    kb = np.array([k0, k0, k], dtype=np.float32)\n",
    "\n",
    "    kf = np.array([kr, kg, kb])\n",
    "    kf = tf.transpose(kf, perm=[2, 3, 1, 0])\n",
    "\n",
    "    kf = tf.constant(kf, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    # apply convolution operation to the images\n",
    "    y = tf.nn.conv2d(imgs, kf, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    img_names = os.listdir(ds_path)\n",
    "    base_path = ds_path + '/../two_by_one_blur/'\n",
    "\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "    \n",
    "    # Save blurred images to new directory two_by_one_blur\n",
    "    for i in range(y.shape[0]):\n",
    "        if img_names[i][-4:] == '.png':\n",
    "            img_path = base_path + img_names[i]\n",
    "            tf.keras.utils.save_img(img_path, y[i])\n",
    "            \n",
    "    print('Blurring process complete! Check the directory')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11cf5ea",
   "metadata": {},
   "source": [
    "## Applying the blurring operation to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = 'SimpleSuperResDeconvTest/high_res/'\n",
    "\n",
    "# NB! uncomment if you don't have the two_by_two_blur folder\n",
    "easy_two_by_one_blur_ds(ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea8fa0",
   "metadata": {},
   "source": [
    "## Visualising input and output data\n",
    "\n",
    "Let's plot the high res and blurred images to double-check that the convolution operation did blur the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6560b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_name = os.listdir('SimpleSuperResDeconvTest/high_res/')[40]\n",
    "\n",
    "img_hres = mpimg.imread('SimpleSuperResDeconvTest/high_res/' + img_name)\n",
    "img_blur = mpimg.imread('SimpleSuperResDeconvTest/two_by_one_blur/' + img_name)\n",
    "\n",
    "print(img_name)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_hres)\n",
    "ax.set_title('High res image')\n",
    "plt.axis('off')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.imshow(img_blur)\n",
    "ax.set_title('Simple blur image')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428e582",
   "metadata": {},
   "source": [
    "## Loading datasets into notebook\n",
    "\n",
    "Now that the input and output data is read we can load both of these datasets into this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fa8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading input data....')\n",
    "x_train = get_imgs_from_dir('SimpleSuperResDeconvTest/two_by_one_blur/')[0]\n",
    "print('Loading output data....')\n",
    "y_train = get_imgs_from_dir('SimpleSuperResDeconvTest/high_res/')[0]\n",
    "print('Data has been successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e55627",
   "metadata": {},
   "source": [
    "## Defining the Deep Learning Model\n",
    "\n",
    "Now that the data has been loaded into the notebook successfully, the deep learning model that will be trained on the data can be defined. We will use TensorFlow Keras functional API to define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a47197",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.reshape(x_train, (1, 256, 256, 1))\n",
    "y_train = tf.reshape(y_train, (1, 256, 256, 1))\n",
    "\n",
    "inputs = keras.Input(shape=(256, 256, 1))\n",
    "outputs = Deconv2D((2, 1), kernel_regularizer=keras.regularizers.l2(0.01))(inputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "        loss='mean_absolute_error',\n",
    "        optimizer=tf.keras.optimizers.SGD()\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6227da",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d7e5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_before = model.layers[1].w.numpy()\n",
    "history = model.fit(x_train, y_train, epochs=100)\n",
    "w_after = model.layers[1].w.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75e33f",
   "metadata": {},
   "source": [
    "## Plotting history and kernels\n",
    "\n",
    "Here we will plot the history of the mean absolute error with respect to the number of epochs. The kernels before and after training will also be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378aa859",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reshaping kernels to their correct shapes\n",
    "pad_w = tf.constant([[0, 0], [1, 0]])\n",
    "\n",
    "w_after_ = tf.pad(w_after, pad_w, mode='CONSTANT', constant_values=1)\n",
    "w_after_ = tf.reshape(w_after_, (2, 1))\n",
    "\n",
    "w_before_ = tf.pad(w_before, pad_w, mode='CONSTANT', constant_values=1)\n",
    "w_before_ = tf.reshape(w_before_, (2, 1))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.table(w_before_.numpy(), loc='center')\n",
    "ax.set_title('kernel before training')\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.table(w_after_.numpy(), loc='center')\n",
    "ax.set_title('kernel after training')\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "ax.set_title('Mean Absolute Error over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a631f2d",
   "metadata": {},
   "source": [
    "## Check if the output was predicted correctly\n",
    "\n",
    "Here we will plot the input data, output data, and what the Deconv NN predicted as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "x_img = x_train\n",
    "y_img = y_train\n",
    "\n",
    "y_pred = model(x_img)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "plt.imshow(x_img[0], cmap='gray')\n",
    "ax.set_title('Low res')\n",
    "plt.axis('off')\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "plt.imshow(y_pred[0], cmap='gray')\n",
    "ax.set_title('Predicted Output')\n",
    "plt.axis('off')\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "plt.imshow(y_img[0], cmap='gray')\n",
    "ax.set_title('High Res')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa1191",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The DNN seems to predict the desired output quite well. It minimises the Mean Absolute Error after a few epochs which is normal when you take into account that this problem it is solving is trivial. Next we will compare it to a simple CNN and see how it fairs against it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bca0f0e",
   "metadata": {},
   "source": [
    "## Checking MAE for DNN output against low res image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a081c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_diff = tf.subtract(y_train, y_pred)\n",
    "y_pred_ae = tf.abs(y_pred_diff)\n",
    "y_pred_mae = tf.reduce_mean(y_pred_ae)\n",
    "\n",
    "low_res_diff = tf.subtract(y_train, x_train)\n",
    "low_res_ae = tf.abs(low_res_diff)\n",
    "low_res_mae = tf.reduce_mean(low_res_ae)\n",
    "\n",
    "\n",
    "print('MAE between high res and DNN OUTPUT:\\t' + str(y_pred_mae))\n",
    "print('MAE between high res and LOW RES:\\t' + str(low_res_mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f09b4aee08e0b08b49e1275ec474d424159b1793a9d200aa98b0e86ea75e1de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
